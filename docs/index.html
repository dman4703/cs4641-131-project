<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mean Reversion Trader</title>
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <style>
        body {
            width: 75% !important;
            max-width: 1250px !important;
        }

        @media (max-width: 768px) {
            body {
                width: 95% !important;
            }
        }
    </style>
</head>
<body>
    <h1>Mean Reversion Trader</h1>
    <p class="author">
        Devon O'Quinn, Shayali Patel, Nicholas Nitsche, Julien Perez, Mutimu Njenga
    </p>

    <h2><u>Introduction</u></h2>
    <p>Mean reversion trading is based on the principle that prices tend to revert toward their average after 
        periods of overextension. To develop a consistent strategy, we leverage machine learning to make 
        predictions on market data while additionally optimizing performance for low-capital intraday trading.</p>

    <h3>Literature Review</h3>
    <p>
        Key features for mean reversion include VWAP, "the price a 'naive' trader can expect to obtain," and 
        anchored VWAP (AVWAP), which tracks trend direction from a user‑set anchor point [1]. Bollinger Bands 
        provide secondary signals of overbought/oversold conditions through volatility envelopes [2]. The triple 
        barrier method labels trades realistically via profit‑taking, stop‑loss, and time limits [3], while 
        event‑based sampling reduces time‑bar heteroskedasticity [3].
    </p>

    <p>
        Gaussian mixture models can be used to determine probabilistic overextension scores from joint features, 
        and adapt better to regime shifts than z‑scores [4]. Random Forests handle nonlinear, correlated data well [5]; 
        paired with sequential bootstrap, there is less label‑overlap bias [3]. The propabilities output by this model
        indicate position sizes and whether to trade/not trade. Gradient‑boosted trees adapted for quantile 
        regression can be used for exit sizing bypredict conditional quantiles for dynamic stops and targets;
        they outperform classical methods in high‑dimensional settings [6][7].
    </p>

    <p>
        Lastly, model evaluation requires careful consideration. Traditional random test/train splits fail for 
        financial data because features and labels are serially correlated [3]. Purged k‑fold with embargo 
        removes overlap between samples, and CPCV returns a distribution of out‑of‑sample metrics from purged 
        k-fold, providing a more reliable measure of model performance [3].
    </p>

    <h3>Dataset Description</h3>
    <p>
        Quantitative, structured intraday time-series for 10–20 liquid U.S. equities (stock selection detailed 
        in Data Preprocessing), collected over 5 trading days during regular hours (excluding the first/last 
        5 minutes) from the Georgia Tech Bloomberg Terminal. These raw inputs consist of trades (price/size), 
        quotes (bid/ask and spread), and accumulated trading volume. We aggregate ticks into volume/dollar bars 
        to equalize information content. Derived features include AVWAP distance (z-score), Bollinger-band 
        position, short-term momentum, relative volume, and time-of-day. We (very tentatively) expect ~150–300 
        bars per session per ticker, totaling ~7.5k–30k bars. We will remove halts, out-of-sequence/duplicate 
        prints, and standardize per asset/session.
    </p>

    <h2><u>Problem Definition</u></h2>

    <h3>Problem & Motivation</h3>
    <p>
        Traditional retail day trading strategies utilize naive heuristic rules that result in inconsistent 
        performance; this problem is amplified in low-capital accounts, where limited funds magnify risk and 
        reduce flexibility. We reinterpret the task as a machine learning problem: How can we better 
        distinguish true trading signals from market noise? Instead of doing subjective guesswork, we can 
        engineer event-based features, label outcomes, and train models. This way, we can realize profitable 
        outcomes with reliable, risk-adjusted performance using statistics rather than rule-of-thumb trading.
    </p>

    <h3>Objective</h3>
    <p>
        Given an overextension event where price is far from an AVWAP, determine whether the price will revert 
        sufficiently within the next 15–30 minutes to yield a profitable mean‑reversion trade. If a profitable 
        reversion is likely, determine where to set dynamic take‑profit and stop‑loss levels to maximize 
        returns by predicting the distribution of return magnitudes and executing the trade accordingly.
    </p>

    <h2><u>Methods</u></h2>

    <h3>Data Preprocessing</h3>
    <ol>
        <li><strong>Stock selection.</strong> For our universe, we will assume a trading capital of $2,500. 
            Stocks will be selected based on the following criteria:
            <ul>
                <li>Price band: $10–$100, so we can buy multiple lots without over-allocating capital.</li>
                <li>Liquidity: average daily volume in the upper 50 percentile.</li>
                <li>Volatility: moderate intraday movement, around 3-5%.</li>
                <li>Diversity: Selected from a variety of sectors.</li>
            </ul>
        </li>
        <li><strong>Bar construction.</strong> Aggregate ticks into volume or dollar bars to normalize 
            information content.</li>
        <li><strong>Feature engineering.</strong> VWAP distance (expressed as a z-score), Bollinger position, 
            short‑term momentum, relative volume, time‑of‑day, and a five‑minute recent trading context feature.</li>
        <li><strong>Normalization.</strong> Standardize per asset and per session for comparability.</li>
        <li><strong>Labeling.</strong> Triple‑barrier labels are used. A trade is successful if the price 
            reverts before the stop or time limit (15–30 minutes).</li>
        <li><strong>Splitting.</strong> Purged k‑fold with embargo to prevent information leakage.</li>
    </ol>

    <h3>Models</h3>
    <ol>
        <li><strong>Unsupervised overextension detector.</strong> GMM on event features. Events whose 
            log‑likelihood falls below a threshold are flagged as overextended candidates.</li>
        <li><strong>Supervised opportunity classifier.</strong> Random Forest traind on the overextension 
            candidates; it outputs the probability of reversion.</li>
        <li><strong>Supervised exit model.</strong> Gradient‑boosted trees (trained on the events predicted 
            to revert with high probability) to predict τ‑quantiles; we will use the 0.1 quantile as the stop 
            loss condition and the 0.5 quantile as the target condition.</li>
    </ol>

    <h2><u>Results & Discussion</u></h2>

    <h3>Metrics</h3>
    <p>For the GMM, we will look at the log-likelihood of events and stability of flagged overextensions. We 
        will report the mean and standard deviation of F1 scores, negative log‑loss from purged k‑fold 
        cross‑validation, and the proportion of events the model chooses to trade for the classifier. For the 
        exit model, we will evaluate the calibration of predicted quantiles and use out‑of‑sample 
        profit‑and‑loss per trade and Sharpe ratio as final metrics. The CPCV procedure produces a 
        distribution of Sharpe ratios, allowing us to compute confidence intervals and assess the stability 
        of the strategy.</p>

    <h3>Project Goals</h3>
    <ul>
        <li>Build an intraday ML pipeline across multiple different asset types.</li>
        <li>Outperform naive heuristic trading.</li>
    </ul>

    <h3>Expected Outcomes</h3>
    <p>We expect the strategy to trade only a small number of overextensions, with a success rate 
        significantly above random. We also expect it to perform better than standard heuristic trading.</p>

    <h2><u>Team Logistics</u></h2>

    <h3>Gantt Chart (Click to enlarge)</h3>
    <a href="./assets/img/ganttchart.png" target="_blank"><img src="./assets/img/ganttchart.png" alt="Inital Proposed Gantt Chart"></a>

    <h3>Contribution Table</h3>
    <table>
        <thead>
            <tr>
                <th>Name</th>
                <th>Proposal Contributions</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Devon O'Quinn</td>
                <td>Literature Review, Proposal Presentation</td>
            </tr>
            <tr>
                <td>Shayali Patel</td>
                <td>Gantt Chart, Objective</td>
            </tr>
            <tr>
                <td>Nicholas Nitsche</td>
                <td>Gantt Chart, Methods</td>
            </tr>
            <tr>
                <td>Julien Perez</td>
                <td>Gantt Chart, Results</td>
            </tr>
            <tr>
                <td>Mutimu Njenga</td>
                <td>Formatting</td>
            </tr>
        </tbody>
    </table>

    <h2><u>References</u></h2>
    <p>[1] B. Shannon, "Anchored VWAP," <em>Alphatrends</em>. [Online]. Available: <a href="https://alphatrends.net/anchored-vwap/" target="_blank">https://alphatrends.net/anchored-vwap/</a></p>

    <p>[2] C. Thompson, "Understanding Bollinger Bands: A Key Technical Analysis Tool for Investors," <em>Investopedia</em>, Sep. 3, 2025. [Online]. Available: <a href="https://www.investopedia.com/terms/b/bollingerbands.asp" target="_blank">https://www.investopedia.com/terms/b/bollingerbands.asp</a></p>

    <p>[3] M. López de Prado, <em>Advances in Financial Machine Learning</em>. Wiley, 2018.</p>

    <p>[4] C. M. Bishop, <em>Pattern Recognition and Machine Learning</em>. New York, NY: Springer, 2006, ch. 9.</p>

    <p>[5] L. Breiman, "Random Forests," <em>Machine Learning</em>, vol. 45, no. 1, pp. 5–32, 2001. DOI: 10.1023/A:1010933404324</p>

    <p>[6] J. H. Friedman, "Greedy Function Approximation: A Gradient Boosting Machine," <em>Annals of Statistics</em>, vol. 29, no. 5, pp. 1189–1232, 2001. DOI: 10.1214/aos/1013203451</p>

    <p>[7] N. Meinshausen, "Quantile Regression Forests," <em>Journal of Machine Learning Research</em>, vol. 7, pp. 983–999, 2006.</p>
</body>
</html>

