<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mean Reversion Trading with ML</title>
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <style>
        body {
            width: 75% !important;
            max-width: 1250px !important;
        }

        @media (max-width: 768px) {
            body {
                width: 95% !important;
            }
        }

        /* Slideshow Styles */
        .slideshow-container {
            position: relative;
            max-width: 800px;
            margin: 20px auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            overflow: hidden;
        }

        .slide {
            display: none;
            text-align: center;
        }

        .slide.active {
            display: block;
        }

        .slide img {
            width: 100%;
            height: auto;
            display: block;
        }

        .slide-caption {
            padding: 10px;
            background-color: #f8f8f8;
            border-top: 1px solid #ddd;
            font-size: 0.9em;
            color: #333;
        }

        .slideshow-nav {
            position: absolute;
            top: 50%;
            transform: translateY(-50%);
            background-color: rgba(0, 0, 0, 0.5);
            color: white;
            border: none;
            padding: 10px 15px;
            cursor: pointer;
            font-size: 18px;
            transition: background-color 0.3s;
        }

        .slideshow-nav:hover {
            background-color: rgba(0, 0, 0, 0.8);
        }

        .prev {
            left: 10px;
        }

        .next {
            right: 10px;
        }

        .dots-container {
            text-align: center;
            padding: 10px;
            background-color: #f8f8f8;
        }

        .dot {
            height: 12px;
            width: 12px;
            margin: 0 4px;
            background-color: #bbb;
            border-radius: 50%;
            display: inline-block;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .dot.active {
            background-color: #333;
        }

        /* Table and Image Styling */
        table {
            margin: 20px auto;
            display: table;
        }

        img {
            display: block;
            margin: 20px auto;
            max-width: 100%;
            height: auto;
        }

        .img-caption {
            text-align: center;
            font-size: 0.9em;
            color: #555;
            font-style: italic;
            margin: -10px auto 20px auto;
            max-width: 800px;
        }
    </style>
</head>
<body>
    <h1>Mean Reversion Trading with Machine Learning</h1>
    <p class="author">
        Devon O'Quinn
    </p>

    <h2><u>Introduction</u></h2>
    <p>Mean reversion trading is based on the principle that prices tend to revert toward their average after 
        periods of overextension. To develop a consistent strategy, we leverage machine learning to make 
        predictions on market data while additionally optimizing performance for low-capital intraday trading.</p>

    <h3>Literature Review</h3>
    <p>
        Key features for mean reversion include VWAP, volume weighted average price, or "the price a 'naive' trader can expect to obtain," and 
        anchored VWAP (AVWAP), which tracks trend direction from a user‑set anchor point [1]. Bollinger Bands 
        provide secondary signals of overbought/oversold conditions through volatility envelopes [2]. The triple 
        barrier method labels trades realistically via profit‑taking, stop‑loss, and time limits [3], while 
        event‑based sampling reduces time‑bar heteroskedasticity [3].
    </p>

    <p>
        Gaussian mixture models can be used to determine probabilistic overextension scores from joint features, 
        and adapt better to regime shifts than z‑scores [4]. Random Forests handle nonlinear, correlated data well [5]; 
        paired with sequential bootstrap, there is less label‑overlap bias [3]. The probabilities output by this model
        indicate position sizes and whether to trade/not trade. Gradient‑boosted trees adapted for quantile 
        regression can be used for exit sizing by predicting conditional quantiles for dynamic stops and targets;
        they outperform classical methods in high‑dimensional settings [6][7].
    </p>

    <p>
        Lastly, model evaluation requires careful consideration. Traditional random test/train splits fail for 
        financial data because features and labels are serially correlated [3]. Purged k‑fold with embargo 
        removes overlap between samples, and combinatorial purged cross-validation (CPCV) returns a distribution of out‑of‑sample metrics from purged 
        k-fold, providing a more reliable measure of model performance [3].
    </p>

    <h3>Dataset Description</h3>
    <p>
        The raw dataset is composed of tick data for 20 liquid U.S. equities, selected from the S&amp;P&nbsp;500 
        using price, liquidity and volatility screens. Data were collected over five trading days (October 2, 3, 6, 7 
        and 8 of 2025) during regular trading hours, excluding the opening and closing five minutes. Each day's file 
        contains millisecond‑timestamped trade prints and quote updates exported from a Bloomberg Terminal. 
        In total the raw universe spans about 21.4 million tick records; after cleaning and consolidation 
        (deduplication, NBBO construction and condition‑code filtering) we have about 1.13 million valid trades 
        across all stocks and days.
    </p>
    <p>
        The 20 selected stocks and their screening statistics are listed below. They cover a broad range of sectors and satisfy our 
        price, liquidity and ATR% criteria. <code>LastClose</code> is the most recent adjusted close price, <code>ADV60d</code> is 
        the mean dollar volume over the previous 60 trading days, and <code>ATRpctMed20</code> is the median 20‑day Average True 
        Range percentage.
    </p>
    <table>
        <caption><strong>Selected Stocks</strong></caption>
        <thead>
            <tr>
                <th>Ticker</th><th>Company</th><th>Sector</th><th>LastClose ($)</th><th>ADV60d ($)</th><th>ATR% (20‑day median)</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>INTC</td><td>Intel</td><td>Information&nbsp;Technology</td><td>37.01</td><td>3.30e9</td><td>4.83</td></tr>
            <tr><td>SMCI</td><td>Supermicro</td><td>Information&nbsp;Technology</td><td>57.69</td><td>1.63e9</td><td>4.13</td></tr>
            <tr><td>TTD</td><td>The Trade Desk</td><td>Communication&nbsp;Services</td><td>53.30</td><td>1.05e9</td><td>4.38</td></tr>
            <tr><td>XYZ</td><td>Block&nbsp;Inc.</td><td>Financials</td><td>80.17</td><td>7.76e8</td><td>3.27</td></tr>
            <tr><td>FCX</td><td>Freeport‑McMoRan</td><td>Materials</td><td>43.15</td><td>7.69e8</td><td>4.67</td></tr>
            <tr><td>CCL</td><td>Carnival</td><td>Consumer&nbsp;Discretionary</td><td>28.70</td><td>5.92e8</td><td>3.07</td></tr>
            <tr><td>MCHP</td><td>Microchip&nbsp;Technology</td><td>Information&nbsp;Technology</td><td>65.64</td><td>5.39e8</td><td>3.09</td></tr>
            <tr><td>KVUE</td><td>Kenvue</td><td>Consumer&nbsp;Staples</td><td>16.53</td><td>4.98e8</td><td>4.39</td></tr>
            <tr><td>CNC</td><td>Centene Corporation</td><td>Health&nbsp;Care</td><td>38.39</td><td>4.76e8</td><td>4.22</td></tr>
            <tr><td>DAL</td><td>Delta Air Lines</td><td>Industrials</td><td>59.70</td><td>4.52e8</td><td>3.00</td></tr>
            <tr><td>ON</td><td>ON Semiconductor</td><td>Information&nbsp;Technology</td><td>50.13</td><td>4.45e8</td><td>3.47</td></tr>
            <tr><td>DLTR</td><td>Dollar Tree</td><td>Consumer&nbsp;Staples</td><td>88.08</td><td>4.34e8</td><td>3.04</td></tr>
            <tr><td>PCG</td><td>PG&amp;E&nbsp;Corp.</td><td>Utilities</td><td>16.53</td><td>3.83e8</td><td>3.41</td></tr>
            <tr><td>DXCM</td><td>Dexcom</td><td>Health&nbsp;Care</td><td>67.80</td><td>3.64e8</td><td>4.58</td></tr>
            <tr><td>DOW</td><td>Dow&nbsp;Inc.</td><td>Materials</td><td>21.94</td><td>3.58e8</td><td>3.74</td></tr>
            <tr><td>NCLH</td><td>Norwegian Cruise Line</td><td>Consumer&nbsp;Discretionary</td><td>23.58</td><td>3.56e8</td><td>3.52</td></tr>
            <tr><td>DECK</td><td>Deckers Brands</td><td>Consumer&nbsp;Discretionary</td><td>99.00</td><td>3.42e8</td><td>3.08</td></tr>
            <tr><td>UBER</td><td>Uber</td><td>Industrials</td><td>98.05</td><td>1.63e9</td><td>2.93</td></tr>
            <tr><td>SLB</td><td>Schlumberger</td><td>Energy</td><td>33.56</td><td>5.62e8</td><td>2.93</td></tr>
            <tr><td>EQT</td><td>EQT Corporation</td><td>Energy</td><td>55.44</td><td>4.47e8</td><td>2.95</td></tr>
        </tbody>
    </table>
    <p>
        After processing, we construct around 600 volume bars per trading day for each ticker, yielding about 46,855 bars with nine 
        derived features and triple‑barrier labels. The cleaning pipeline achieved an average NBBO coverage of 99.99%, 
        zero negative spreads, and an average duplicate removal rate of 33.07%. The final cleaned dataset contains 1,130,327 trades across all 
        tickers and days.
    </p>
    <table>
        <caption><strong>Cleaned Data Schema</strong></caption>
        <thead>
            <tr>
                <th>Column</th><th>Type</th><th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>ts</td><td>datetime64[ns, UTC]</td><td>Trade timestamp in UTC</td></tr>
            <tr><td>ticker</td><td>object</td><td>Stock ticker symbol</td></tr>
            <tr><td>type</td><td>category</td><td>Event type (TRADE)</td></tr>
            <tr><td>price</td><td>float32</td><td>Trade execution price</td></tr>
            <tr><td>size</td><td>int32</td><td>Trade size in shares</td></tr>
            <tr><td>cond</td><td>object</td><td>Raw trade condition codes</td></tr>
            <tr><td>cond_norm</td><td>object</td><td>Normalized condition codes</td></tr>
            <tr><td>exch</td><td>category</td><td>Exchange identifier</td></tr>
            <tr><td>nbb</td><td>float32</td><td>National best bid price</td></tr>
            <tr><td>nbo</td><td>float32</td><td>National best offer price</td></tr>
            <tr><td>nbb_size</td><td>int32</td><td>National best bid size</td></tr>
            <tr><td>nbo_size</td><td>int32</td><td>National best offer size</td></tr>
            <tr><td>mid</td><td>float32</td><td>Mid-price: 0.5 × (nbb + nbo)</td></tr>
            <tr><td>spread</td><td>float32</td><td>Bid-ask spread: nbo − nbb</td></tr>
            <tr><td>at_bid</td><td>int64</td><td>Flag: trade at bid (1) or not (0)</td></tr>
            <tr><td>at_ask</td><td>int64</td><td>Flag: trade at ask (1) or not (0)</td></tr>
        </tbody>
    </table>


    <h2><u>Problem Definition</u></h2>

    <h3>Problem & Motivation</h3>
    <p>
        Traditional retail day trading strategies utilize naive heuristic rules that result in inconsistent 
        performance; this problem is amplified in low-capital accounts, where limited funds magnify risk and 
        reduce flexibility. We reinterpret the task as a machine learning problem: How can we better 
        distinguish true trading signals from market noise? Instead of doing subjective guesswork, we can 
        engineer event-based features, label outcomes, and train models. This way, we can realize profitable 
        outcomes with reliable, risk-adjusted performance using statistics rather than rule-of-thumb trading.
    </p>

    <h3>Objective</h3>
    <p>
        Given an overextension event where price is far from an AVWAP, determine whether the price will revert 
        sufficiently within the next 15–30 minutes to yield a profitable mean‑reversion trade. If a profitable 
        reversion is likely, determine where to set dynamic take‑profit and stop‑loss levels to maximize 
        returns by predicting the distribution of return magnitudes and executing the trade accordingly.
    </p>

    <h2><u>Methods</u></h2>

    <h3>Data Preprocessing</h3>
    <p>
        <b>Stock Selection.</b> Tickers were drawn from the S&P 500 and filtered for last close between $10 and $100, belonging to 
        the top half of the dollar‑volume distribution, and exhibiting moderate intraday volatility (ATR% between 3% and 5%). 
        When fewer than 20 tickers satisfied these strict criteria we filled the remainder with names closest to the volatility 
        midpoint while still prioritizing liquidity.
    </p>
    <table>
        <caption><strong>Data Cleaning Metrics</strong></caption>
        <thead>
            <tr>
                <th>Metric</th><th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Raw tick records</td><td>21,419,919</td></tr>
            <tr><td>Final cleaned trades</td><td>1,130,327</td></tr>
            <tr><td>Retention rate</td><td>5.3%</td></tr>
            <tr><td>Avg duplicate removal</td><td>33.07%</td></tr>
            <tr><td>Avg NBBO coverage</td><td>99.99%</td></tr>
            <tr><td>Avg crossed quotes</td><td>0.52%</td></tr>
            <tr><td>Trades dropped (condition codes)</td><td>2,005,302</td></tr>
            <tr><td>Trades dropped (no NBBO)</td><td>140</td></tr>
        </tbody>
    </table>
    <p>
        <b>Cleaning</b>. Bloomberg headers and invalid rows were removed, New York timestamps were parsed to UTC, exact repeats were 
        deduplicated, trades were split from quotes and a National Best Bid and Offer (NBBO) was built by forward‑filling quotes 
        up to 2 seconds. The NBBO construction ensures trades are contextualized against the prevailing market allowing the 
        computation of accurate spreads and assessment of price impact; the 2‑second forward‑fill cap prevents using stale quotes that would 
        misrepresent market conditions. Each trade was merged with the most recent valid NBBO; special condition codes (late reports, 
        auctions, odd lots, extended hours, etc.) were filtered out because these irregular trade types do not reflect normal 
        market liquidity and would introduce bias into our mean‑reversion features. Compressed Parquet files were exported. Quality 
        gates ensured >99% NBBO coverage, zero negative spreads and chronologically ordered timestamps. Average duplicate removal was 
        roughly 33% of raw rows and NBBO crossed quotes occurred in less than 1% of cases.
    </p>
    <p>
        <b>Bar Type</b>. To determine whether to use volume or dollar bars, we performed EDA on three representative tickers. By examining per‑second 
        flow statistics (coefficient of variation, Fano factor, intraday profiles and zero‑activity periods) we found that 
        dollar‑denominated flows were slightly more bursty and less uniform than share‑based flows. Prototype bars built on dollar 
        thresholds also exhibited more variable inter‑bar durations and less white noise in returns. Consequently we adopted volume 
        bars with ticker‑specific thresholds calibrated as median daily volume divided by 600 (clamped between 2,000 and 100,000 shares). 
        This yields approximately 600 bars per day per ticker; each bar captures the OHLC, VWAP, trade count, duration and volume.
    </p>
    <table>
        <caption><strong>Volume Bar Configuration (Selected Tickers)</strong></caption>
        <thead>
            <tr>
                <th>Ticker</th><th>Volume Threshold (shares/bar)</th><th>Target Bars/Day</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>INTC</td><td>20,500</td><td>~600</td></tr>
            <tr><td>PCG</td><td>8,700</td><td>~600</td></tr>
            <tr><td>KVUE</td><td>7,500</td><td>~600</td></tr>
            <tr><td>SMCI</td><td>6,800</td><td>~600</td></tr>
            <tr><td>CCL</td><td>4,600</td><td>~600</td></tr>
            <tr><td>FCX</td><td>4,600</td><td>~600</td></tr>
            <tr><td>TTD</td><td>2,400</td><td>~600</td></tr>
            <tr><td>NCLH</td><td>2,300</td><td>~600</td></tr>
            <tr><td>CNC</td><td>2,200</td><td>~600</td></tr>
            <tr><td>SLB</td><td>2,200</td><td>~600</td></tr>
            <tr><td>DAL</td><td>2,000</td><td>~600</td></tr>
            <tr><td>Others</td><td>2,000</td><td>~600</td></tr>
        </tbody>
    </table>
    <img src="./assets/img/determineBar_8_1.png" alt="Three bar charts compare Volume vs Dollar across INTC, SMCI, FCX on 10-2-25 and 10-3-25. Left: CV similar. 
    Middle: Dollar shows dramatically higher Fano Factor (more bursty). Right: Zero-activity rates are close; FCX highest (~75–78%), INTC lowest (~55–60%).">
    <p class="img-caption">Figure 1: Comparison of volume vs. dollar bar metrics.</p>
    <p>
        <b>Feature Engineering</b>. From each volume bar we engineered nine features while avoiding look‑ahead bias. These features 
        best capture mean‑reversion signals: VWAP z‑score and Bollinger position directly measure price 
        overextension from mean levels, which is the overarching signal for mean reversion. Three‑ and five‑bar momentum quantify short‑term 
        trend strength that may reverse. Relative volume indicates unusual activity that often precedes reversions. Time of day 
        accounts for intraday liquidity patterns. The three context variables (bar count, 
        average volume, and price range over the past five minutes) provide microstructure information about recent market conditions 
        that affect reversion probability. All rolling statistics are shifted by one bar to ensure the features do not incorporate 
        the current bar's outcome, strictly preventing look‑ahead bias. The feature distributions are approximately symmetric with 
        moderate tails.
    </p>
    <table>
        <caption><strong>Engineered Features</strong></caption>
        <thead>
            <tr>
                <th>Feature</th><th>Description</th><th>Formula / Method</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>VWAP z-score</td><td>Standardized distance from VWAP</td><td>(close − VWAP) / rolling_std(20).shift(1)</td></tr>
            <tr><td>Bollinger position</td><td>Position within Bollinger Bands</td><td>(close − BB_mid) / (BB_upper − BB_mid), 20-bar window, 2σ</td></tr>
            <tr><td>Momentum (3-bar)</td><td>Log return over 3 bars</td><td>log(close / close.shift(3))</td></tr>
            <tr><td>Momentum (5-bar)</td><td>Log return over 5 bars</td><td>log(close / close.shift(5))</td></tr>
            <tr><td>Relative volume</td><td>Volume vs. rolling median</td><td>volume / rolling_median(20).shift(1)</td></tr>
            <tr><td>Time of day</td><td>Normalized time (0=open, 1=close)</td><td>(t − market_open) / market_duration</td></tr>
            <tr><td>Context: bar count</td><td>Number of bars in past 5 minutes</td><td>count(bars where t ≥ current_t − 5min)</td></tr>
            <tr><td>Context: avg volume</td><td>Average volume in past 5 minutes</td><td>mean(volume where t ≥ current_t − 5min)</td></tr>
            <tr><td>Context: price range</td><td>Price range in past 5 minutes</td><td>(max(high) − min(low)) / mean(close)</td></tr>
        </tbody>
    </table>
    <img src="./assets/img/dataEDA_10_0.png" alt="Grid of nine histograms summarizing features: VWAP z-score and Momentum (3/5 bar) roughly normal around zero; Bollinger position centered near zero; Relative volume clustered near 1 
    with right-tail outliers; time-of-day roughly uniform; context bar count, average volume, and price range strongly right-skewed.">
    <p class="img-caption">Figure 2: Distribution of the nine engineered features.</p>
    <p>
        <b>Labeling</b>. Labels are derived using the triple‑barrier method with volatility‑scaled price barriers and a 20‑minute time horizon. 
        This approach is preferred over fixed‑price targets or fixed‑time horizons because it adapts to changing volatility regimes 
        and realistically models how traders exit positions—either hitting a profit target, hitting a stop loss, or timing out. We 
        compute an exponentially‑weighted rolling standard deviation of returns (halflife = 50 bars) and set upper and lower 
        barriers at ±1× this volatility from the entry price, ensuring that profit/loss thresholds scale with recent market conditions. 
        A bar receives a label of <code>+1</code> if the upper barrier is hit first, <code>-1</code> if the lower barrier is hit, 
        or <code>0</code> if the time barrier elapses without either price barrier being breached. This reduces path‑dependent 
        labeling bias and produces more realistic training signals than pure forward returns. Across our processed dataset, up and down 
        labels are roughly balanced while neutral events are rare. Holding periods cluster around 30–100 seconds.
    </p>
    <img src="./assets/img/dataEDA_6_0.png" alt="Two bar charts. Left: labels −1, 0, 1—down and up are nearly equal (~360 each); neutral is rare (~10). 
    Right: first barrier hit—lower and upper occur most often and are similar; time-out is uncommon (~5).">
    <p class="img-caption">Figure 3: Triple-barrier label distribution across a data sample.</p>
    <p>
        <b>Cross-Validation</b>. Finally, we partitioned the data using purged, embargoed cross‑validation. Traditional random splits 
        are inappropriate for financial time‑series because both features and labels exhibit serial correlation: a bar's outcome can 
        influence nearby bars through autocorrelation, shared volatility regimes, and overlapping triple‑barrier windows. In a 
        leave‑one‑day‑out (LODO) scheme, we hold out one trading day for testing and train on the remaining days. We purge any 
        training samples whose entry or exit windows overlap the validation period and enforce a 20‑minute embargo after the validation 
        cut‑off to prevent information leakage. This approach ensures that validation performance reflects 
        out‑of‑sample generalization rather than memorization of temporally adjacent patterns. This yields five folds for the 
        five trading days; in each fold about 7,000–10,000 bars are reserved for validation while the rest form the training set.
    </p>

    <h3>Models</h3>

    <h4>GMM: Unsupervised Overextension Detector</h4>
    <p>
        To identify overextended price moves without supervision, we fit a GMM to the 
        nine‑dimensional feature vectors constructed from the volume bars. We chose an unsupervised approach because labeling 
        "overextension" directly is subjective and would require hindsight bias; instead, we let the model learn the natural 
        distribution of feature patterns and flag anomalies. A GMM models the data as a weighted sum of k 
        components, where each component represents a distinct market regime or pattern in the 
        feature space. We test 8 to 15 components to balance model flexibility against complexity. The GMM assigns each point a 
        probability density: "normal" events receive high probability under the learned distribution, while 
        low‑probability outliers in the tails correspond to unusual, potentially overextended situations worthy of further analysis. 
        We prefer GMMs over simpler alternatives (e.g., univariate z‑scores or Isolation Forest) for several reasons: GMMs 
        capture correlations between features (e.g., VWAP z‑score and Bollinger position often move together), 
        they model multimodality in the feature space, allowing different "normal" regimes to coexist, and 
        they adapt to regime shifts by blending multiple covariance structures rather than assuming a single Gaussian. 
        This provides interpretable anomaly scores and handles the non‑stationary nature of intraday trading data.
    </p>
    <p>
        The model is trained on four of the five trading days (October 2, 3, 6, 7) and tested on the remaining day (October 8). 
        Prior to training we drop rows with missing or infinite feature values (400 training samples, 100 test samples) and standardize 
        each feature in the training set. We tune the number of mixture components 
        (<code>k</code>) and covariance type (<code>full</code> vs. <code>diag</code>) via day‑level LODO cross‑validation on the 
        training data. Model selection follows two steps: first, we identify configurations within one standard deviation 
        of the best cross‑validated log‑likelihood; second, among these candidates we choose the model with the lowest Bayesian Information 
        Criterion (BIC). The selected configuration uses <code>k = 12</code> components with full covariance, achieving a mean cross‑validated 
        log‑likelihood of –7.25 and the lowest BIC (523,136) among models within one standard deviation of the best CV score.
    </p>
    <table>
        <caption><strong>GMM Configuration and Model Selection</strong></caption>
        <thead>
            <tr>
                <th>Parameter</th><th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Number of components (k)</td><td>12</td></tr>
            <tr><td>Covariance type</td><td>Full</td></tr>
            <tr><td>Training samples</td><td>37,793</td></tr>
            <tr><td>Test samples</td><td>8,562</td></tr>
            <tr><td>Feature dimensionality</td><td>9</td></tr>
            <tr><td>CV mean log-likelihood</td><td>–7.25</td></tr>
            <tr><td>Train AIC</td><td>517,508</td></tr>
            <tr><td>Train BIC</td><td>523,136</td></tr>
            <tr><td>Threshold quantile (primary)</td><td>5th percentile</td></tr>
            <tr><td>Threshold value (5%)</td><td>–14.18</td></tr>
            <tr><td>Threshold value (10%)</td><td>–12.05</td></tr>
        </tbody>
    </table>
    <p>
        After fitting, we compute the per‑sample log‑likelihood under the learned GMM. Events with log‑likelihood below the 
        5th percentile of the training distribution are flagged as overextension candidates. The threshold (–14.18) is chosen 
        purely from the train distribution. For future comparison and sensitivity analysis, we also persist candidates using a 
        10th percentile threshold (–12.05). These flags are not final trading signals; they simply mark the subset of bars that 
        may warrant further analysis by the supervised opportunity classifier. For interpretability we also record the mixture 
        component assignments, which can reveal clusters of feature patterns associated with extremes.
    </p>
    <img src="./assets/img/gmm_15_0.png" alt="Line chart titled 'GMM model selection (LODO CV)' showing CV mean log-likelihood versus number of components (2–8). 
    Two lines: cov=full consistently higher than cov=diag; both improve (less negative) as components increase.">
    <p class="img-caption">Figure 4: GMM model selection via LODO cross-validation showing full covariance models consistently outperform diagonal covariance.</p>

    <h4>Supervised Opportunity Classifier</h4>
    <p>
        Once the GMM flags overextension candidates, we need to distinguish which of these anomalies are likely to 
        revert profitably versus those that represent genuine regime shifts or continuation moves. For this task we 
        train a supervised Random Forest binary classifier on the GMM‑flagged subset of bars (5th percentile threshold;
         2,347 samples across all days). The target is binary: label = +1 (profitable mean reversion, 
        upper barrier hit first) versus label ∈ {−1, 0} (no profitable reversion). This reframes the problem from 
        detecting anomalies to predicting reversion success.
    </p>
    <p>
        We chose Random Forest over alternative classifiers for several reasons. First, Random Forests handle 
        nonlinear, correlated features naturally without requiring explicit feature transformations or regularization. 
        Our nine features likely interact in complex ways that tree ensembles can capture. Second, Random Forests are robust to 
        outliers and require minimal hyperparameter tuning compared to gradient boosting or neural networks, making 
        them a practical baseline for this moderately sized dataset. Third, they provide interpretable feature 
        importances, which helps diagnose which signals drive reversion predictions. Finally, 
        the probabilistic outputs can be directly used for risk sizing and thresholding 
        in downstream trading logic.
    </p>
    <p>
        The model is trained on four days (October 2, 3, 6, 7; 1,890 samples) and tested on the fifth day 
        (October 8; 457 samples). Class balance is approximately 50/50 for both train and test splits. We tune 
        hyperparameters via day‑level LODO cross‑validation on the training set, searching over number of trees 
        (200 vs. 400), maximum depth (None vs. 10), minimum samples per leaf (5 vs. 10), and feature sampling 
        strategy (sqrt vs. all features). The configuration is selected by maximizing mean F1 score across the 
        four LODO folds. The winning configuration uses 400 trees, unrestricted depth, minimum 5 samples per leaf, 
        and considers all features at each split.
    </p>
    <table>
        <caption><strong>Random Forest Configuration</strong></caption>
        <thead>
            <tr>
                <th>Parameter</th><th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Number of estimators</td><td>400</td></tr>
            <tr><td>Maximum depth</td><td>None (unrestricted)</td></tr>
            <tr><td>Min samples per leaf</td><td>5</td></tr>
            <tr><td>Max features per split</td><td>All (9 features)</td></tr>
            <tr><td>Training samples</td><td>1,890</td></tr>
            <tr><td>Test samples</td><td>457</td></tr>
            <tr><td>Feature dimensionality</td><td>9</td></tr>
            <tr><td>Train class balance</td><td>50.9% positive, 49.1% negative</td></tr>
            <tr><td>Test class balance</td><td>51.4% positive, 48.6% negative</td></tr>
        </tbody>
    </table>

    <h4>Supervised Exit Model</h4>
    <p>
        Once the Random Forest classifies an overextension event as likely to revert, the next step is determining 
        where to set dynamic exit levels: a stop‑loss to limit downside and a profit target to capture gains. 
        Rather than using fixed price thresholds or volatility multiples, we train Gradient‑Boosted quantile regressors 
        to predict the conditional distribution of forward returns (the price change from entry to exit). This approach has several advantages over traditional 
        methods: it adapts exit levels dynamically based on current market conditions (captured by the same nine features), 
        it provides probabilistic bounds rather than point estimates, and it can capture nonlinear relationships between 
        features and return quantiles.
    </p>
    <p>
        We chose Gradient Boosting Machines (GBM) for quantile regression over alternatives (linear quantile regression, 
        quantile forests) for several reasons. First, GBMs handle nonlinear feature interactions effectively through 
        sequential tree construction, which is important given the complex relationships between volatility, momentum, 
        and expected returns. Second, scikit‑learn's <code>GradientBoostingRegressor</code> natively supports quantile 
        loss (pinball loss), making implementation straightforward. Third, GBMs are computationally efficient for our 
        moderately sized dataset (1,424 training samples) and provide interpretable feature importances. Fourth, the 
        regularization parameters (tree depth, learning rate, minimum samples per leaf) help prevent overfitting to 
        noise in the return distribution.
    </p>
    <p>
        We train two separate GBM models to predict different quantiles of the forward return distribution:
    </p>
    <ul>
        <li><b>Q10 Model (10th Percentile)</b>: Predicts the lower tail of the return distribution, serving as a 
        dynamic stop‑loss level. By targeting the 10th percentile, we expect approximately 10% of realized returns 
        to fall below this predicted value, giving a safe risk floor.</li>
        <li><b>Q50 Model (50th Percentile / Median)</b>: Predicts the median expected return, serving as a profit 
        target. This represents the "typical" outcome conditional on the features and provides a realistic exit 
        point.</li>
    </ul>
    <p>
        The models are trained on events that the Random Forest predicts will revert (RF probability ≥ 0.3), using 
        the same day‑level Leave‑One‑Day‑Out (LODO) cross‑validation scheme on four training days (October 2, 3, 6, 7) 
        and tested on the held‑out day (October 8). Hyperparameters are tuned via grid search over the number of 
        estimators, tree depth, learning rate, and minimum samples per leaf, selecting configurations that minimize 
        mean cross‑validated pinball loss.
    </p>
    <table>
        <caption><strong>GBM Model Configuration</strong></caption>
        <thead>
            <tr>
                <th>Parameter</th><th>Q10 (Stop-Loss)</th><th>Q50 (Target)</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Model Type</td><td>GradientBoostingRegressor</td><td>GradientBoostingRegressor</td></tr>
            <tr><td>Loss Function</td><td>quantile (alpha=0.1)</td><td>quantile (alpha=0.5)</td></tr>
            <tr><td>n_estimators</td><td>200</td><td>100</td></tr>
            <tr><td>max_depth</td><td>3</td><td>3</td></tr>
            <tr><td>learning_rate</td><td>0.05</td><td>0.05</td></tr>
            <tr><td>min_samples_leaf</td><td>20</td><td>20</td></tr>
            <tr><td>Train Samples</td><td>1,424</td><td>1,424</td></tr>
            <tr><td>Test Samples</td><td>417</td><td>417</td></tr>
            <tr><td>Features</td><td>9</td><td>9</td></tr>
        </tbody>
    </table>
    <p>
        Both models converged to shallow trees (depth 3) and conservative learning rates (0.05), suggesting that 
        the return distribution is noisy and benefits from regularization. The Q10 model required more estimators 
        (200 vs. 100) to capture the lower‑tail dynamics, which are inherently sparser and more variable than 
        the median. The figure below shows the distribution of forward returns used as targets. The left histogram reveals 
        a bimodal pattern with distinct peaks on either side of zero. This is a consequence of the triple‑barrier 
        labeling, where trades typically exit at profit‑taking or stop‑loss levels rather than at arbitrary prices. 
        This bimodality motivates the use of quantile regression rather than mean regression, as the conditional 
        mean would fall between the modes and represent neither typical outcome.
    </p>
    <img src="./assets/img/gbm_7_0.png" alt="Distribution of forward returns used as targets for GBM quantile regression. The left panel displays the aggregated distribution for all RF-flagged overextension events, 
    revealing a distinct bimodal structure centered at zero with most returns falling within ±40 basis points. The right panel stratifies these returns by their triple-barrier outcome, identifying the source of this bimodality: 
    Label −1 events (blue) cluster in negative territory (stop-losses), while Label +1 events (green) cluster in positive territory (profit-taking). Label 0 events (orange) are rare. This separation confirms that the volatility-based 
    barriers force trades into distinct outcomes, motivating the use of quantile regression to model these specific tails rather than a conditional mean.">
    <p class="img-caption">Bimodal forward returns reflecting distinct stop-loss and profit-taking outcomes from triple-barrier labeling.</p>

    <h2><u>Results & Discussion</u></h2>

    <h3>GMM: Unsupervised Overextension Detector</h3>
    <p>
        <b>Model Selection and Information Criteria</b>. We evaluated GMM configurations with 8 to 15 components and two covariance types 
        (full and diagonal) using day‑level LODO cross‑validation. Model selection was guided by two constraints: first, we identify models 
        within one standard deviation of the best cross‑validated log‑likelihood; second, among these candidates we select the configuration 
        with the lowest Bayesian Information Criterion (BIC). These metrics are particularly well‑suited for unsupervised GMM evaluation: 
        unlike supervised metrics (accuracy, F1), they do not require ground‑truth labels and instead measure how well the model explains 
        the observed data distribution. This prevents overfitting to validation noise while favoring simpler models 
        when performance is comparable, ensuring the chosen GMM generalizes to new trading days.
    </p>
    <p>
        <b>Log-Likelihood</b>. In an unsupervised setting without ground-truth labels, 
        log‑likelihood is the primary metric for evaluating how well the GMM explains the observed data distribution. It represents the 
        average log‑probability that the fitted GMM assigns to each data point. Higher (less negative) values indicate the model 
        assigns higher probability to the observed feature patterns, meaning it better captures the underlying structure. For example, 
        a mean log‑likelihood of –6.8 is better than –8.0 because the former indicates the model finds the data more probable under its 
        learned distribution. We evaluate log‑likelihood on held‑out validation days to assess generalization: if a model fits the training 
        distribution well but produces lower log‑likelihood on validation data, it has likely overfit. Conversely, similar train and test log‑likelihoods 
        confirm that the model has learned generalizable patterns.
    </p>
    <p>
        Among the 16 configurations evaluated, k=15 full covariance achieved the best CV log‑likelihood (–7.15), but k=12 full covariance was selected due to its 
        substantially lower BIC (523,136) compared to k=15 (530,430), despite a marginal CV difference of only 0.10. AIC (Akaike Information 
        Criterion) and BIC are information‑theoretic metrics that balance goodness‑of‑fit against model 
        complexity; they are ideal for unsupervised GMM selection because they penalize overly complex models that may fit noise rather than 
        signal. BIC applies a stronger penalty than AIC for additional parameters, making it more conservative. Lower AIC and BIC 
        values indicate better models: the 7,000‑point BIC improvement of k=12 over k=15 indicates that three fewer components provide a 
        better parsimony trade‑off—k=15's slight CV advantage does not justify its added complexity. Note that AIC and BIC values 
        scale linearly with sample size (n = 37,793), so absolute magnitudes in the hundreds of thousands are expected; what matters for 
        model comparison is the relative difference between candidates. The diagonal‑covariance models consistently performed worse, 
        suggesting that feature correlations (e.g., between VWAP z-score and Bollinger position) are important for accurately modeling 
        the joint distribution.
    </p>
    <table>
        <caption><strong>GMM Model Selection Results (Top 5 Configurations)</strong></caption>
        <thead>
            <tr>
                <th>k</th><th>Covariance</th><th>CV Mean Log-Lik</th><th>CV Std</th><th>BIC</th><th>AIC</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>15</td><td>full</td><td>–7.15</td><td>0.49</td><td>530,430</td><td>523,393</td></tr>
            <tr><td>12</td><td>full</td><td>–7.25</td><td>0.37</td><td>523,136</td><td>517,508</td></tr>
            <tr><td>13</td><td>full</td><td>–7.26</td><td>0.51</td><td>524,511</td><td>518,414</td></tr>
            <tr><td>11</td><td>full</td><td>–7.30</td><td>0.49</td><td>536,847</td><td>531,689</td></tr>
            <tr><td>14</td><td>full</td><td>–7.31</td><td>0.62</td><td>524,836</td><td>518,269</td></tr>
        </tbody>
    </table>
    <p>
        <b>Generalization and Candidate Flagging</b>. On the fifth (test) day (October 8), the fitted k=12 GMM produced a mean log‑likelihood 
        of –6.86 compared with –6.83 on the training days. These values are nearly identical, confirming that the model generalizes well: 
        it explains the test day's feature distribution about as well as it explains the training data. This similarity suggests that the model 
        has learned patterns that persist across different trading days rather than overfitting. 
        If the test log‑likelihood had been substantially lower (more negative), it would indicate the model failed to capture generalizable structure.
    </p>
    <p>
        Using the 5th percentile threshold (–14.18), about 5.0% of training bars and 5.3% of test bars were 
        flagged as overextension candidates. This stable candidate rate across train and test confirms that the threshold defined on the training 
        distribution transfers reliably to new data. Additionally, we persist a 10th percentile threshold (–12.05) 
        for future sensitivity analysis and comparison with the stricter 5% threshold. The flags represent anomalous feature combinations 
        that fall in the low‑density tails of the learned distribution, marking potential mean‑reversion opportunities for downstream supervised analysis.
    </p>
    <table>
        <caption><strong>GMM Performance</strong></caption>
        <thead>
            <tr>
                <th>Metric</th><th>Train</th><th>Test</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Sample count</td><td>37,793</td><td>8,562</td></tr>
            <tr><td>Mean log-likelihood</td><td>–6.83</td><td>–6.86</td></tr>
            <tr><td>Candidate rate (5%)</td><td>5.00%</td><td>5.34%</td></tr>
            <tr><td>Candidates flagged (5%)</td><td>1,890</td><td>457</td></tr>
        </tbody>
    </table>
    <p>
        A breakdown of flagged vs. unflagged labels shows that both up and down events are represented in 
        roughly equal proportions: in the train set the flagged subset contained 963 positive and 896 negative labels, while the 
        unflagged subset contained 18,099 positive and 17,432 negative labels. Neutral (label 0) events are rare overall (≈1%) and 
        only a handful are flagged.
    </p>
    <table>
        <caption><strong>Label Distribution by Flag Status (Training Set, 5%)</strong></caption>
        <thead>
            <tr>
                <th>Label</th><th>Flagged Count</th><th>Flagged %</th><th>Unflagged Count</th><th>Unflagged %</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Up (+1)</td><td>963</td><td>51.0%</td><td>18,099</td><td>50.5%</td></tr>
            <tr><td>Down (–1)</td><td>896</td><td>47.4%</td><td>17,432</td><td>48.6%</td></tr>
            <tr><td>Neutral (0)</td><td>31</td><td>1.6%</td><td>372</td><td>1.0%</td></tr>
            <tr><td>Total</td><td>1,890</td><td>100%</td><td>35,903</td><td>100%</td></tr>
        </tbody>
    </table>
    <p>
        <b>Interpretability and Feature Space Analysis</b>. The log‑likelihood distributions overlap between train and test and flagged 
        points cluster in low‑density regions when projected into the first two principal components. The flagged bars often correspond 
        to large deviations from VWAP, extreme Bollinger positions, high relative volume or unusual momentum patterns, consistent with 
        our intuition of overextended price moves. However, because the GMM operates on the joint feature space, some moderate deviations 
        can also be flagged if they occur in an atypical feature combination. The model does not assign explicit probabilities to labels; 
        instead, the flags simply identify potential mean‑reversion opportunities for further screening.
    </p>

    <div class="slideshow-container" id="slideshow-gmm">
        <div class="slide active">
            <img src="./assets/img/gmm_16_5.png" alt="Overlapping histograms of GMM log-likelihoods for train (blue) and test (orange). Most mass near 0 with a thin left tail. 
            A vertical red dashed line around −14 marks the threshold. Title: 'Log-likelihood distributions with threshold.'' Axes: log-likelihood (x), density (y).">
            <div class="slide-caption">Figure 5a: Overlapping log-likelihood distributions for train and test sets with 5th percentile threshold marked.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/gmm_16_2.png" alt="Bar chart 'GMM candidate rate by ticker — 2025-10-08.'' Vertical bars sorted descending. PCG highest (~21%), then DECK (~16%), 
            INTC (~14%), SMCI (~10%). Middle tickers around 3–7%. Several near 1–2% (TTD, KVUE, MCHP, ON, SLB, UBER, DOW).">
            <div class="slide-caption">Figure 5b: GMM candidate rate by ticker on test day showing variation across stocks.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/gmm_15_2.png" alt="Scatter plot of test PCA projection (PC1 vs PC2). Blue dots = not flagged; orange dots = GMM-flagged. Blue cluster dense near center;
             flagged points sparsely distributed toward the outer edges and along the negative PC2 tail. Title: 'Test PCA (colored by GMM candidate flag)'.">
            <div class="slide-caption">Figure 5c: PCA projection showing flagged candidates (orange) cluster in low-density regions away from normal events (blue).</div>
        </div>
        <div class="slide">
            <img src="./assets/img/gmm_16_6.png" alt="Scatter plot of test-day points: x = feat_vwap_zscore, y = feat_bollinger_position. Blue (not flagged) form a dense, positively sloped ellipse around (0,0). 
            Orange (flagged) are sparse, mostly at extremes and outer edges. Legend shows categories. Title notes date 2025-10-08.">
            <div class="slide-caption">Figure 5d: VWAP z-score vs. Bollinger position scatter plot with flagged events.</div>
        </div>
        <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">&#10094;</button>
        <button class="slideshow-nav next" onclick="changeSlide(this, 1)">&#10095;</button>
        <div class="dots-container">
            <span class="dot active" onclick="currentSlide(this, 0)"></span>
            <span class="dot" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
        </div>
    </div>

    <p>
        <b>Next Steps</b>. While the candidate rate of ~5% is a reasonable starting point, several avenues exist to refine the unsupervised detector. 
        First, the feature set could be expanded to include additional context such as order‑book imbalance or realized volatility. 
        Second, alternative density estimators (e.g., Kernel Density Estimation or Normalizing Flows) might capture heavy tails and 
        nonlinear dependencies better than Gaussian mixtures. Third, the threshold could be adaptively set per ticker or per day to 
        account for differing volatility regimes. Despite these limitations, the current GMM provides a principled, interpretable 
        filter that reduces the search space for the supervised models and establishes a baseline for anomaly detection in 
        high‑frequency trading data.
    </p>

    <h3>Supervised Opportunity Classifier</h3>
    <p>
        <b>Cross-Validation and Hyperparameter Selection</b>. We tuned 4 hyperparameters using 
        day‑level leave‑one‑day‑out (LODO) cross‑validation on the four training days. Each fold held out one day 
        for validation while training on the remaining three days. The selected model (400 trees, 
        no depth limit, 5 samples per leaf, all features) achieved a mean CV F1 of 50.9% and mean ROC‑AUC of 52.8%, 
        marginally outperforming alternatives. All tested configurations produced very similar performance 
        (F1 ranging from 47.4% to 50.9%), suggesting that the prediction task is inherently difficult and that 
        hyperparameter choices have limited impact. 
    </p>
    <!-- <table>
        <caption><strong>Random Forest Hyperparameter Tuning (Top 5 by Mean F1)</strong></caption>
        <thead>
            <tr>
                <th>n_estimators</th><th>max_depth</th><th>min_samples_leaf</th><th>max_features</th>
                <th>Mean Accuracy</th><th>Mean Precision</th><th>Mean Recall</th><th>Mean F1</th><th>Mean ROC-AUC</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>400</td><td>None</td><td>5</td><td>All</td><td>0.512</td><td>0.531</td><td>0.512</td><td>0.509</td><td>0.528</td></tr>
            <tr><td>400</td><td>10</td><td>5</td><td>All</td><td>0.508</td><td>0.530</td><td>0.520</td><td>0.507</td><td>0.527</td></tr>
            <tr><td>200</td><td>10</td><td>5</td><td>All</td><td>0.505</td><td>0.527</td><td>0.509</td><td>0.502</td><td>0.522</td></tr>
            <tr><td>200</td><td>None</td><td>5</td><td>sqrt</td><td>0.504</td><td>0.524</td><td>0.503</td><td>0.499</td><td>0.524</td></tr>
            <tr><td>400</td><td>10</td><td>10</td><td>All</td><td>0.505</td><td>0.523</td><td>0.518</td><td>0.498</td><td>0.528</td></tr>
        </tbody>
    </table> -->
    <p>
        Per‑fold metrics for the best configuration reveal moderate variability across validation days: accuracy 
        ranged from 49.6% to 53.2%, recall from 35.1% to 67.3%, and F1 from 43.3% to 55.1%. This heterogeneity 
        suggests that reversion predictability varies by day, potentially due to differing intraday volatility regimes, 
        news events, or liquidity conditions. The standard deviations 
        (accuracy ±1.5%, recall ±13.8%, F1 ±5.6%) confirm that day‑to‑day performance is unstable.
    </p>
    <table>
        <caption><strong>LODO CV Metrics by Validation Day</strong></caption>
        <thead>
            <tr>
                <th>Validation Day</th><th>n_val</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1</th><th>ROC-AUC</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Oct 2</td><td>707</td><td>0.496</td><td>0.564</td><td>0.351</td><td>0.433</td><td>0.510</td></tr>
            <tr><td>Oct 3</td><td>268</td><td>0.511</td><td>0.550</td><td>0.462</td><td>0.502</td><td>0.534</td></tr>
            <tr><td>Oct 6</td><td>361</td><td>0.532</td><td>0.542</td><td>0.562</td><td>0.552</td><td>0.539</td></tr>
            <tr><td>Oct 7</td><td>554</td><td>0.509</td><td>0.466</td><td>0.673</td><td>0.551</td><td>0.529</td></tr>
            <tr><td><b>Mean ± Std</b></td><td>—</td><td>0.512 ± 0.015</td><td>0.531 ± 0.044</td><td>0.512 ± 0.138</td><td>0.509 ± 0.056</td><td>0.528 ± 0.013</td></tr>
        </tbody>
    </table>
    <p>
        <b>Test Performance</b>. On the test day, the Random Forest achieved an accuracy 
        of 52.5%, precision of 54.7%, recall of 44.3%, F1 of 48.9%, and ROC‑AUC of 53.6%. These metrics are 
        consistent with the cross‑validation results, confirming that the model generalizes to unseen data without 
        significant overfitting. However, the absolute performance is weak: an ROC‑AUC of 53.6% indicates the model 
        ranks profitable reversions only modestly better than random chance.
    </p>
    <table>
        <caption><strong>Test Day Performance</strong></caption>
        <thead>
            <tr>
                <th>Metric</th><th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Accuracy</td><td>52.5%</td></tr>
            <tr><td>Precision</td><td>54.7%</td></tr>
            <tr><td>Recall</td><td>44.3%</td></tr>
            <tr><td>F1 Score</td><td>48.9%</td></tr>
            <tr><td>ROC-AUC</td><td>53.6%</td></tr>
            <tr><td>True Negatives</td><td>136</td></tr>
            <tr><td>False Positives</td><td>86</td></tr>
            <tr><td>False Negatives</td><td>131</td></tr>
            <tr><td>True Positives</td><td>104</td></tr>
        </tbody>
    </table>
    <p>
        The ROC and precision–recall curves illustrate the limited discriminatory power. The ROC curve hugs the diagonal pretty well, 
        reflecting the weak AUC. The precision–recall curve shows that at very high probability thresholds (top‑left corner) the model can achieve near‑perfect precision, 
        but this corresponds to extremely low recall - only a handful of trades would be executed. As the threshold 
        is relaxed, precision quickly drops to the 0.50–0.55 range while recall increases. This tradeoff is 
        characteristic of a weak classifier: there is no "sweet spot" where both precision and recall are high.
    </p>
    <img src="./assets/img/randomForest_6_0.png" alt="The figure shows the out-of-sample performance of the Random Forest opportunity classifier on the held-out test day (October 8, 2025). 
    The left panel plots the ROC curve, where the model’s blue line lies only modestly above the diagonal “Random” baseline, yielding an ROC AUC of 0.536; this indicates a weak but positive ability
     to rank profitable reversions above non-reversions. The right panel shows the corresponding precision–recall curve: at very high probability thresholds the classifier attains near-perfect precision 
     but at extremely low recall (only a handful of trades), and as the threshold is relaxed, precision quickly stabilizes in the 0.50–0.55 range while recall increases toward 1. Overall, the curves suggest 
     that while the model does extract some predictive signal from the features, its discriminatory power is limited and any trading strategy built on these scores would need to rely on careful thresholding and 
     risk management rather than expecting strong standalone classification accuracy.">
    <p class="img-caption">Figure 6: ROC and precision–recall curves on the held-out test day. The modest separation from baseline indicates weak but non-zero predictive power.</p>
    <p>
        <b>Feature Importances</b>. Gini‑based feature importances reveal that <code>feat_time_of_day</code> 
        is the single most influential variable (15.4%), indicating that when in the trading session an overextension 
        occurs strongly affects reversion likelihood. The next most 
        important features are <code>feat_context_price_range</code> (13.5%) and <code>feat_context_avg_volume</code> 
        (12.5%), which capture recent intraday volatility and local trading activity. Their prominence suggests that 
        reversions are more predictable when the candidate occurs within specific short‑horizon volatility and liquidity 
        regimes. Core mean‑reversion signals—<code>feat_vwap_zscore</code> (11.1%) and <code>feat_bollinger_position</code> 
        (11.0%)—rank fourth and fifth, reflecting how far price is from recent value and band extremes. Secondary 
        contributors include <code>feat_relative_volume</code> (10.8%), short‑horizon momentum measures 
        (<code>feat_momentum_5bar</code> 9.3%, <code>feat_momentum_3bar</code> 8.5%), and <code>feat_context_bar_count</code> 
        (7.9%). The relatively flat importance distribution (no single feature dominates beyond 16%) suggests that 
        reversion prediction requires a mix of time‑of‑day structure, local volatility/volume context, and 
        distance‑from‑value indicators rather than relying on any one strong signal.
    </p>
    <img src="./assets/img/randomForest_7_2.png" alt="This figure shows the top Random Forest feature importances for the opportunity classifier, measured using Gini importance over the training days. 
    feat_time_of_day is the single most influential variable, indicating that when in the trading session an overextension occurs strongly affects whether it subsequently mean-reverts. The next group of 
    features—feat_context_price_range and feat_context_avg_volume—captures the recent intraday volatility and local trading activity, suggesting that reversions are more predictable when the candidate occurs 
    within specific short-horizon volatility and liquidity regimes. Microstructure and positioning signals such as feat_vwap_zscore and feat_bollinger_position also carry substantial weight, reflecting how far price 
    is from recent value and band extremes at the moment the GMM flags an overextension. Secondary but still meaningful contributors include feat_relative_volume, short-horizon momentum measures (feat_momentum_5bar, feat_momentum_3bar), 
    and the feat_context_bar_count, reinforcing the idea that the classifier relies primarily on a mix of time-of-day structure, local volatility/volume context, and distance-from-value indicators rather than on pure trend alone.">
    <p class="img-caption">Figure 7: Gini-based feature importances. Time-of-day, local volatility context, and price positioning signals are most influential.</p>
    <p>
        <b>Calibration, Probability Distributions, and Threshold Analysis</b>. Figure 8a shows the model is reasonably 
        well‑calibrated (quantile bins somewhat near the diagonal) with probabilities clustering between 0.15–0.70. The 
        confusion matrix reveals roughly balanced predictions across all four cells at the 0.5 threshold. 
        Figure 8b visualizes substantial overlap between class probability distributions: the no‑reversion class has mean 
        0.451 while the reversion class has mean 0.474, confirming weak separation and 
        explaining the near‑uniform confusion matrix. Figure 8c explores threshold tradeoffs, revealing the optimal F1 (0.679) 
        occurs at an impractically low threshold of 0.02 (98.3% recall, 51.9% precision); at the default 0.5 threshold, 
        performance is more balanced but still weak (54.7% precision, 44.3% recall, 48.9% F1). Accuracy hovers around 50–52.5% 
        across all thresholds, barely above random guessing. The analysis reveals no operating point achieves both high precision 
        and high recall, confirming the model lacks strong predictive signal and requires better features, more data, or 
        alternative approaches.
    </p>

    <div class="slideshow-container" id="slideshow-rf">
        <div class="slide active">
            <img src="./assets/img/randomForest_8_0.png" alt="This figure summarizes the reliability and hard-decision performance of the Random Forest opportunity classifier on the held-out test day (October 8, 2025). 
            The left panel shows a calibration curve, where the model’s predicted probabilities (x-axis) are grouped into quantile bins and compared against the empirical reversion frequency in each bin (y-axis). Most points 
            fall in the 0.15–0.70 probability range, reflecting that the classifier rarely issues extremely confident predictions; they cluster around the diagonal “perfectly calibrated” line, with modest oscillations that indicate 
            slight underestimation of risk at lower scores and occasional overconfidence at higher scores, but no systematic miscalibration. The right panel displays the 2×2 confusion matrix at the default 0.5 decision threshold: the 
            model correctly classifies 136 non-reversion events and 104 reversion events, while mislabeling 86 non-reversions as reversions and 131 reversions as non-reversions. With total counts roughly balanced across the four cells, 
            the confusion matrix reinforces the picture from the calibration plot—a reasonably calibrated but only weakly discriminative classifier, whose probabilities are usable as a soft ranking signal but whose binary decisions 
            provide limited separation between profitable and non-profitable opportunities.">
            <div class="slide-caption">Figure 8a: Calibration curve and confusion matrix on test day. The model is well-calibrated but weakly discriminative.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/randomForest_9_1.png" alt="This figure visualizes how the Random Forest’s predicted probabilities are distributed for each class on the held-out test day. The left panel overlays histograms of 
            predicted reversion probabilities for non-reversion events (red) and reversion events (green), with the default decision threshold of 0.5 shown as a vertical dashed line. Both distributions are centered near the threshold 
            and strongly overlap, but the green bars are slightly shifted to the right, indicating that true reversions tend to receive somewhat higher scores on average even though the separation is modest. The right panel shows the 
            same information as side-by-side boxplots: the median predicted probability for reversion cases lies just above 0.5, while the non-reversion median lies just below, with overlapping interquartile ranges and several low-probability 
            outliers in both groups. The dashed horizontal line again marks the 0.5 cutoff, highlighting that the classifier mainly operates in a narrow 0.4–0.6 probability band and provides only weak discrimination between the two classes.">
            <div class="slide-caption">Figure 8b: Predicted probability distributions by class showing substantial overlap between reversion and non-reversion events.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/randomForest_10_0.png" alt="This figure illustrates how the Random Forest’s classification performance on the held-out test day varies with the decision threshold. The left panel plots 
            precision, recall, and F1 score as functions of the threshold applied to the predicted reversion probabilities. At very low thresholds almost all candidates are classified as reversions, yielding recall near 1.0 
            but only moderate precision around 0.5; the F1 curve peaks at an optimal threshold of roughly 0.02 (marked by the red dot), where the balance between precision and recall gives an F1 score of about 0.68. As the threshold 
            increases toward the default 0.5 (vertical dashed line), recall drops sharply while precision improves only slightly, causing F1 to decline steadily. The right panel shows accuracy versus threshold, with the random-guess 
            baseline at 0.5 indicated by a dotted line. Accuracy remains fairly flat at just above 0.5 across a wide range of thresholds, including both the default 0.5 and the F1-optimal 0.02 (red dot), confirming that no single cutoff 
            yields a strong improvement in overall hit rate, and that threshold choice mainly trades off precision against recall rather than dramatically boosting accuracy.">
            <div class="slide-caption">Figure 8c: Precision, recall, F1, and accuracy vs. decision threshold. No operating point achieves both high precision and high recall.</div>
        </div>
        <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">&#10094;</button>
        <button class="slideshow-nav next" onclick="changeSlide(this, 1)">&#10095;</button>
        <div class="dots-container">
            <span class="dot active" onclick="currentSlide(this, 0)"></span>
            <span class="dot" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
        </div>
    </div>
    <p>
        <b>Why Does the Model Perform Weakly?</b> Several factors likely explain the limited predictive power. 
        First, the sample size is small: with only 1,890 training samples and 457 test samples (after GMM filtering 
        to 5%), there is insufficient data for the Random Forest to learn robust decision boundaries, especially 
        given the nine‑dimensional feature space and high noise in intraday price movements. Second, the binary 
        target (profitable reversion vs. no profitable reversion) may be inherently noisy because mean reversion 
        is a probabilistic phenomenon influenced by unobserved factors such as order flow, latent news, and 
        microstructure effects not captured by our features. Third, the features themselves may lack sufficient 
        signal: while VWAP z‑score and Bollinger position measure overextension, they do not incorporate directional 
        information about order book imbalance, bid‑ask dynamics, or recent trade intensity, all of which may 
        improve reversion prediction. Fourth, the 5% GMM threshold may flag a too "unique" set of anomalies (some 
        are genuine overextensions, while others are regime shifts or continuation moves) diluting the supervised 
        signal.
    </p>
    <p>
        <b>Next Steps and Potential Improvements</b>. To improve the classifier, we could expand the feature set to 
        include order‑book imbalance, realized volatility, and trade‑level microstructure signals (e.g., signed trade 
        intensity). We could also explore alternative models: gradient boosting (XGBoost, LightGBM) might better 
        handle interactions and small sample sizes, and calibrated ensembles  
        could refine probability estimates. Sequential bootstrap or sample weighting could mitigate label overlap 
        bias from the triple‑barrier method. Additionally, we could stratify by ticker or volatility regime to train 
        ticker‑specific or regime‑specific models, acknowledging that reversion dynamics may vary across stocks. 
        Despite these limitations, the current Random Forest provides a good baseline and its calibrated 
        probabilities offer a soft signal for downstream exit modeling and risk management. The weak but positive 
        discriminatory power suggests that the features contain some reversion‑related information, 
        even if it is insufficient for strong standalone classification.
    </p>

    <h3>Supervised Exit Model</h3>
    <p>
        <b>Cross-Validation and Hyperparameter Selection</b>. We tuned both quantile models using day‑level LODO 
        cross‑validation on the four training days. Each fold held out one day for validation while training on the 
        remaining three days, selecting hyperparameters that minimize mean cross‑validated pinball loss - the 
        asymmetric loss function that penalizes under-predictions by τ and over-predictions by (1−τ), where lower 
        values indicate better quantile estimation. The Q10 model achieved a CV mean pinball loss of 0.000202 ± 0.000005, 
        demonstrating highly stable performance across folds. The Q50 model achieved a CV mean pinball loss of 
        0.000469 ± 0.000066, showing more variability, reflecting the inherent difficulty in predicting median returns 
        compared to downside tails.
    </p>
    <table>
        <caption><strong>GBM Performance Summary</strong></caption>
        <thead>
            <tr>
                <th>Metric</th><th>Q10 (Stop-Loss)</th><th>Q50 (Target)</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>CV Mean Pinball Loss</td><td>0.000202</td><td>0.000469</td></tr>
            <tr><td>CV Std Pinball Loss</td><td>0.000005</td><td>0.000066</td></tr>
            <tr><td>Train Pinball Loss</td><td>0.000177</td><td>0.000418</td></tr>
            <tr><td>Test Pinball Loss</td><td>0.000133</td><td>0.000481</td></tr>
            <tr><td>Test Coverage Rate</td><td>9.6%</td><td>66.2%</td></tr>
            <tr><td>Expected Coverage</td><td>10.0%</td><td>50.0%</td></tr>
            <tr><td>Calibration Error</td><td>0.004</td><td>0.162</td></tr>
        </tbody>
    </table>
    <p>
        <b>Test Day Performance</b>. On the held‑out test day (October 8), the models exhibited contrasting behavior. 
        The Q10 model achieved near‑perfect calibration: the test coverage rate (the fraction of actual returns 
        falling below the predicted quantile) of 9.6% is very close to the expected 10%, with a calibration error 
        (|actual − expected| coverage) of only 0.004. This means that approximately 10% of actual returns fell below the 
        predicted stop‑loss level. Remarkably, the test pinball loss (0.000133) was lower than the training loss (0.000177), 
        suggesting that downside risk was particularly predictable on the test day.
    </p>
    <p>
        In contrast, the Q50 model showed significant miscalibration on the test day. The coverage rate of 66.2% 
        exceeded the expected 50%, resulting in a calibration error of 0.162. This means that realized returns 
        fell below the predicted median target 66% of the time instead of 50% - the model was systematically 
        optimistic about potential upside. The test pinball loss (0.000481) increased slightly from training 
        (0.000418), indicating modest generalization degradation.
    </p>
    <img src="./assets/img/gbm_23_0.png" alt="This figure assesses model calibration by comparing expected quantile coverage (green) against realized training (blue) and testing (orange) rates. 
    The Q10 Stop-Loss model (left) demonstrates exceptional stability, with a test coverage of 9.6% nearly identical to the theoretical 10%, confirming precise downside risk estimation. Conversely, 
    the Q50 Target model (right) reveals significant misalignment on the test set; the 66.2% coverage far exceeds the expected 50%. This implies that realized returns fell below the predicted median more 
    frequently than statistically expected, indicating the model was overly optimistic about potential upside on the test day.">
    <p class="img-caption">Figure 9: Coverage rate calibration for both quantile models. Q10 achieves near-perfect calibration while Q50 overestimates upside potential on the test day.</p>
    <p>
        <b>Predictions vs. Actual Returns</b>. The scatter plots below visualize how well each model's predictions 
        align with realized returns on the test day. The "double cluster" pattern in both plots results 
        from the triple‑barrier labeling method, where trades predominantly exit at volatility-scaled price barriers 
        (profit-taking or stop-loss) rather than at the time horizon. For the Q10 model, the cluster on the diagonal represents stopped‑out trades where 
        realized loss matched the prediction, while the upper cluster represents profitable holds that exceeded 
        the risk floor. For the Q50 model, the diagonal cluster marks successful profit‑taking, while the lower 
        off‑diagonal cluster comprises failed reversions that fell short of the predicted median target.
    </p>
    <img src="./assets/img/gbm_21_0.png" alt="This figure evaluates the Gradient Boosted Quantile Regressors on the held-out test day (October 8, 2025). The 'double cluster' pattern in both plots results from the triple-barrier 
    labeling method, where trades force-exit at specific volatility bands or time limits rather than continuously. The left panel (Q10 Stop-Loss) achieves precise 9.6% coverage; the diagonal cluster represents stopped-out trades where 
    realized loss matched the prediction, while the upper cluster represents profitable holds. Conversely, the right panel (Q50 Target) shows 66.2% coverage; the diagonal cluster marks successful profit-taking, while 
    the lower off-diagonal cluster comprises failed reversions that fell short of the predicted median.">
    <p class="img-caption">Figure 10: Quantile predictions vs. actual returns on test day. The double-cluster pattern reflects triple-barrier exit dynamics.</p>
    <p>
        <b>Dynamic Exit Corridors</b>. Figure 11 visualizes the prediction bands produced by the two quantile models, 
        with samples sorted by realized return. The blue shaded region represents the expected trading range bounded 
        by Q10 (stop‑loss) and Q50 (profit target). The fluctuating width demonstrates the model's ability to adapt 
        risk limits dynamically to changing market volatility—wider bands during volatile conditions and tighter 
        bands during calmer periods. Because most trades exit at price barriers rather than timing out, the bimodal 
        return distribution creates distinct clusters at the extremes. The black line (actual returns) breaks through 
        the lower Q10 boundary only at the extreme left, indicating precise downside calibration. Conversely, the 
        black line remains within the band for much of the right side, visually illustrating why Q50 coverage was 
        higher than expected: many profitable trades failed to reach the model's targets.
    </p>
    <img src="./assets/img/gbm_27_0.png" alt="This figure visualizes the dynamic trade management corridors predicted for the test day. Samples are sorted by realized return (black line) 
    to display the distribution from maximum loss (left) to maximum profit (right). The blue shaded region represents the expected trading range, bounded by the predicted Q10 stop-loss and Q50 
    profit target. The fluctuating width of this band demonstrates the model's ability to adapt risk limits dynamically to changing market volatility. The black line punctures the lower Q10 boundary 
    only at the far left, confirming precise downside calibration. Conversely, the black line stays within the band for much of the right side, visually illustrating why the Q50 target coverage (66.2%) 
    was higher than expected—many profitable trades failed to reach the model's ambitious targets.">
    <p class="img-caption">Figure 11: Dynamic exit corridors showing Q10–Q50 prediction bands. The varying width reflects volatility-adaptive risk limits.</p>
    <p>
        <b>Feature Importances</b>. Unlike the Random Forest entry model where time‑of‑day and price positioning features 
        dominated, the GBM exit models rely almost exclusively on "contextual" features. Specifically, 
        <code>feat_time_of_day</code>, <code>feat_context_bar_count</code>, and <code>feat_context_price_range</code> 
        collectively account for the vast majority of predictive power in both models. This indicates that dynamic 
        stop and target levels are driven primarily by prevailing market volatility and liquidity conditions rather 
        than the specific degree of price overextension (e.g., <code>feat_vwap_zscore</code>). Essentially, while 
        price extremes signal when to trade, the market environment (time and recent activity) dictates 
        how far the price is likely to move.
    </p>
    <img src="./assets/img/gbm_25_0.png" alt="This figure illustrates the feature importances for the Gradient Boosted Quantile Regressors. Unlike the entry model where price location 
    was significant, the exit models rely almost exclusively on 'contextual' features—specifically `feat_time_of_day`, `feat_context_bar_count`, and `feat_context_price_range`—which collectively 
    account for the vast majority of predictive power. This indicates that dynamic stop and target levels are driven primarily by the prevailing market volatility and liquidity conditions 
    rather than the specific degree of price overextension (e.g., `feat_vwap_zscore`). Essentially, while price extremes signal *when* to trade, the market environment (time and recent activity) 
    dictates *how far* the price is likely to move.">
    <p class="img-caption">Figure 12: Feature importances reveal that exit levels depend primarily on market context (time, volatility, activity) rather than price positioning signals.</p>
    <p>
        <b>Why Did the Models Perform Differently?</b> The divergent performance between Q10 and Q50 models reveals 
        asymmetries in return predictability. The Q10 model's better calibration suggests that 
        downside risk (the probability of hitting a stop‑loss) is relatively stable and predictable from market 
        context features. This is because volatility regimes and liquidity conditions strongly 
        influence how far prices can move against a position.
    </p>
    <p>
        The Q50 model's overestimation of upside can be attributed to several factors. First, the training 
        period (October 2–7) may have exhibited stronger mean‑reversion dynamics than the test day (October 8), 
        causing the model to learn optimistic targets that didn't generalize. Second, predicting the median 
        is inherently harder than predicting tail quantiles because it requires capturing the central tendency 
        of a noisy, potentially multimodal distribution. Third, the test day may have experienced regime 
        shifts (news events, changed market sentiment) that reduced reversion magnitudes 
        without triggering stop‑losses.
    </p>
    <p>
        <b>Implications for Trading</b>. The calibration results have important implications for live trading:
    </p>
    <ul>
        <li><b>Risk Management</b>: The well‑calibrated Q10 stop‑loss can be used directly for position sizing 
        and risk limits; knowing that approximately 10% of trades will hit the predicted floor provides a 
        reliable worst‑case estimate.</li>
        <li><b>Profit Targets</b>: The Q50 targets should be treated as optimistic estimates. In practice, 
        traders might scale targets down by a factor (e.g., 0.8×) or use a lower quantile (Q40 or Q30) as 
        the profit target to improve calibration.</li>
        <li><b>Dynamic Adjustment</b>: The varying band widths suggest that exit levels should be recalculated 
        as market conditions evolve within a trade, not just at entry.</li>
    </ul>
    <p>
        <b>Next Steps and Potential Improvements</b>. To improve the exit model, incorporating additional features such as order‑book imbalance or intraday 
        volatility forecasts might improve Q50 calibration. Additionally, ensemble methods combining multiple quantile 
        models or conformalized quantile regression could provide better uncertainty estimates. The 
        0.3 RF probability threshold for sample selection could be tuned jointly with the GBM hyperparameters 
        to optimize downstream trading performance. Despite the Q50 miscalibration, the overall framework 
        demonstrates that GBM quantile regression provides a principled approach to dynamic 
        exit level prediction for mean‑reversion trading.
    </p>
    
    <h3>Full Pipeline</h3>
    <p>
        The full trading pipeline was evaluated on the held‑out test day (October 8, 2025) using 417 trades that passed 
        both the GMM overextension filter and the RF probability threshold (≥ 0.3). Trade direction was determined by 
        VWAP z‑score: negative = LONG (expecting price to rise toward VWAP), positive = SHORT (expecting price to fall 
        toward VWAP). Entry occurs when RF probability exceeds the threshold, and exit is based on the triple‑barrier 
        outcome using GBM‑predicted Q10 stop‑loss and Q50 profit target levels.
    </p>
    <table>
        <caption><strong>Full Pipeline Performance</strong></caption>
        <thead>
            <tr>
                <th>Metric</th><th>Value</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Total Trades</td><td>417</td></tr>
            <tr><td>Wins / Losses / Timeouts</td><td>218 / 195 / 4</td></tr>
            <tr><td>Hit Rate</td><td>52.3%</td></tr>
            <tr><td>Win/Loss Ratio</td><td>1.03</td></tr>
            <tr><td>Total P&L</td><td>0.027854 (278.54 bps)</td></tr>
            <tr><td>Mean P&L per Trade</td><td>0.000067</td></tr>
            <tr><td>Sharpe Ratio</td><td>1.32</td></tr>
        </tbody>
    </table>
    <p>
        <b>Interpretation</b>. The hit rate (the fraction of trades that reached the profit target before 
        hitting the stop‑loss or timing out) came in at 52.3%, meaning just over half of all predicted mean‑reversion 
        opportunities successfully closed at a profit. The win/loss ratio, which compares the average magnitude 
        of winning trades to losing trades, was 1.03; this indicates that when we win, we capture slightly more return 
        than we give back when we lose, a necessary condition for profitability when hit rates hover near 50%. 
        Total P&L(profit and loss) measures the cumulative return across all trades: at 0.027854, or 
        equivalently 278.54 basis points, the pipeline generated meaningful positive returns on the test day. Finally, 
        the Sharpe ratio, a risk‑adjusted performance measure calculated asthe mean return divided by the standard deviation of returns, 
        was 1.32. A Sharpe above 1.0 is generally considered strong for intraday 
        strategies, suggesting that the pipeline's gains are not merely a product of excessive risk‑taking. Taken 
        together, these metrics demonstrate that the combined GMM → RF → GBM pipeline produces profitable, 
        risk‑controlled signals on out‑of‑sample data.
    </p>

    <div class="slideshow-container" id="slideshow-fullpipeline">
        <div class="slide active">
            <img src="./assets/img/fullPipeline_12_0.png" alt="Cumulative intraday performance of the full mean‑reversion pipeline on the held‑out test day (2025‑10‑08). Each point 
            represents a single trade, ordered chronologically, with the green line showing cumulative P&L in log‑return space. Green markers denote trades that hit 
            the GBM‑based profit target, while red markers correspond to stop‑outs. The early drawdown is followed by a strong recovery and a steady late‑session grind 
            higher that produces positive total P&L and a Sharpe ratio of 1.32.">
            <div class="slide-caption">Figure 13a: Cumulative P&L over trades showing early drawdown followed by recovery and steady late-session gains.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/fullPipeline_13_0.png" alt="Distribution of per‑trade returns by exit outcome. The left panel shows histograms for wins (green), losses (red), and 
            timeouts (gray), with a vertical blue line marking the slightly positive overall mean return. Winning trades cluster tightly around small positive returns, 
            while losing trades form a wider, clearly separated cluster of modest negatives. The right panel displays boxplots, highlighting that median wins exceed 
            median losses in magnitude.">
            <div class="slide-caption">Figure 13b: Distribution of trade returns by outcome showing histograms and boxplots for wins, losses, and timeouts.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/fullPipeline_14_0.png" alt="Scatter plots evaluating how well the GBM quantile exit model's predictions align with realized trade 
            returns. The left panel compares actual returns to the predicted Q10 stop‑loss level: losing trades (red) cluster just below the negative quantile, 
            while winning trades (green) stay comfortably above zero. The right panel compares actual returns to the predicted Q50 target: winners cluster around 
            and above the positive median forecast, whereas losses sit below zero.">
            <div class="slide-caption">Figure 13c: Scatter plots comparing actual returns to predicted Q10 stop-loss and Q50 target levels.</div>
        </div>
        <div class="slide">
            <img src="./assets/img/fullPipeline_15_0.png" alt="Per‑ticker performance on the test day. The left panel plots hit rate with a dashed 
            vertical line at the 50% breakeven threshold. Green bars indicate tickers with hit rates above 50%, while red bars mark underperformers. 
            SMCI, DXCM, XYZ and KVUE show especially strong win fractions. The right panel shows corresponding total P&L by ticker, revealing that a 
            handful of names generate most of the day's profits.">
            <div class="slide-caption">Figure 13d: Per-ticker performance showing hit rate and total P&L by ticker.</div>
        </div>
        <button class="slideshow-nav prev" onclick="changeSlide(this, -1)">&#10094;</button>
        <button class="slideshow-nav next" onclick="changeSlide(this, 1)">&#10095;</button>
        <div class="dots-container">
            <span class="dot active" onclick="currentSlide(this, 0)"></span>
            <span class="dot" onclick="currentSlide(this, 1)"></span>
            <span class="dot" onclick="currentSlide(this, 2)"></span>
            <span class="dot" onclick="currentSlide(this, 3)"></span>
        </div>
    </div>

    <p>
        <b>Per‑Ticker Breakdown</b>. Performance varied substantially across tickers. The table below shows the top and 
        bottom performers sorted by total P&L. SMCI, DXCM, and XYZ contributed the most profit with hit rates exceeding 
        60%, while INTC, DLTR, and ON were the largest detractors. Half of the 20 tickers (10/20) achieved positive P&L, 
        and 8/20 exceeded the 50% hit rate threshold.
    </p>
    <table>
        <caption><strong>Performance by Ticker (Sorted by Total P&L)</strong></caption>
        <thead>
            <tr>
                <th>Ticker</th><th>Trades</th><th>Wins</th><th>Losses</th><th>Hit Rate</th><th>Total P&L</th><th>Sharpe</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>SMCI</td><td>79</td><td>48</td><td>31</td><td>60.8%</td><td>0.026667</td><td>2.36</td></tr>
            <tr><td>DXCM</td><td>15</td><td>13</td><td>2</td><td>86.7%</td><td>0.011981</td><td>3.85</td></tr>
            <tr><td>XYZ</td><td>13</td><td>10</td><td>3</td><td>76.9%</td><td>0.008798</td><td>2.00</td></tr>
            <tr><td>PCG</td><td>143</td><td>75</td><td>68</td><td>52.4%</td><td>0.002838</td><td>0.31</td></tr>
            <tr><td>CCL</td><td>11</td><td>6</td><td>5</td><td>54.5%</td><td>0.001907</td><td>0.54</td></tr>
            <tr><td>KVUE</td><td>4</td><td>3</td><td>1</td><td>75.0%</td><td>0.001561</td><td>1.01</td></tr>
            <tr><td>FCX</td><td>16</td><td>8</td><td>8</td><td>50.0%</td><td>0.000835</td><td>0.22</td></tr>
            <tr><td>UBER</td><td>1</td><td>1</td><td>0</td><td>100.0%</td><td>0.000572</td><td>—</td></tr>
            <tr><td>TTD</td><td>5</td><td>2</td><td>3</td><td>40.0%</td><td>0.000157</td><td>0.07</td></tr>
            <tr><td>MCHP</td><td>2</td><td>1</td><td>1</td><td>50.0%</td><td>0.000029</td><td>0.01</td></tr>
            <tr><td>DAL</td><td>12</td><td>7</td><td>5</td><td>58.3%</td><td>−0.000039</td><td>−0.01</td></tr>
            <tr><td>SLB</td><td>1</td><td>0</td><td>1</td><td>0.0%</td><td>−0.000679</td><td>—</td></tr>
            <tr><td>CNC</td><td>10</td><td>5</td><td>5</td><td>50.0%</td><td>−0.000763</td><td>−0.24</td></tr>
            <tr><td>DECK</td><td>13</td><td>4</td><td>5</td><td>30.8%</td><td>−0.000946</td><td>−0.16</td></tr>
            <tr><td>DOW</td><td>2</td><td>0</td><td>2</td><td>0.0%</td><td>−0.001113</td><td>−66.23</td></tr>
            <tr><td>NCLH</td><td>10</td><td>4</td><td>6</td><td>40.0%</td><td>−0.001573</td><td>−0.61</td></tr>
            <tr><td>EQT</td><td>9</td><td>4</td><td>5</td><td>44.4%</td><td>−0.001988</td><td>−0.43</td></tr>
            <tr><td>ON</td><td>3</td><td>0</td><td>3</td><td>0.0%</td><td>−0.003084</td><td>−6.79</td></tr>
            <tr><td>DLTR</td><td>8</td><td>2</td><td>6</td><td>25.0%</td><td>−0.005091</td><td>−1.60</td></tr>
            <tr><td>INTC</td><td>60</td><td>25</td><td>35</td><td>41.7%</td><td>−0.012217</td><td>−1.53</td></tr>
        </tbody>
    </table>
    <p>
        <b>Summary</b>. The full pipeline demonstrates that combining unsupervised anomaly detection (GMM) with 
        supervised classification (RF) and quantile‑based exit prediction (GBM) produces a viable mean‑reversion 
        trading strategy. The positive Sharpe ratio and overall profitability on the out‑of‑sample test day validate 
        the pipeline architecture, though per‑ticker variability suggests that future work could incorporate 
        ticker‑specific models or dynamic ticker selection to improve consistency.
    </p>

    <h2><u>References</u></h2>
    <p>[1] B. Shannon, "Anchored VWAP," <em>Alphatrends</em>. [Online]. Available: <a href="https://alphatrends.net/anchored-vwap/" target="_blank">https://alphatrends.net/anchored-vwap/</a></p>

    <p>[2] C. Thompson, "Understanding Bollinger Bands: A Key Technical Analysis Tool for Investors," <em>Investopedia</em>, Sep. 3, 2025. [Online]. Available: <a href="https://www.investopedia.com/terms/b/bollingerbands.asp" target="_blank">https://www.investopedia.com/terms/b/bollingerbands.asp</a></p>

    <p>[3] M. López de Prado, <em>Advances in Financial Machine Learning</em>. Wiley, 2018.</p>

    <p>[4] C. M. Bishop, <em>Pattern Recognition and Machine Learning</em>. New York, NY: Springer, 2006, ch. 9.</p>

    <p>[5] L. Breiman, "Random Forests," <em>Machine Learning</em>, vol. 45, no. 1, pp. 5–32, 2001. DOI: 10.1023/A:1010933404324</p>

    <p>[6] J. H. Friedman, "Greedy Function Approximation: A Gradient Boosting Machine," <em>Annals of Statistics</em>, vol. 29, no. 5, pp. 1189–1232, 2001. DOI: 10.1214/aos/1013203451</p>

    <p>[7] N. Meinshausen, "Quantile Regression Forests," <em>Journal of Machine Learning Research</em>, vol. 7, pp. 983–999, 2006.</p>

    <script>
        // Track current slide index for each slideshow
        const slideshowStates = {};

        function changeSlide(element, direction) {
            const container = element.closest('.slideshow-container');
            const containerId = container.id;
            
            // Initialize state if needed
            if (!(containerId in slideshowStates)) {
                slideshowStates[containerId] = 0;
            }
            
            const slides = container.querySelectorAll('.slide');
            const dots = container.querySelectorAll('.dot');
            const currentIndex = slideshowStates[containerId];
            
            slides[currentIndex].classList.remove('active');
            dots[currentIndex].classList.remove('active');
            
            slideshowStates[containerId] = (currentIndex + direction + slides.length) % slides.length;
            
            slides[slideshowStates[containerId]].classList.add('active');
            dots[slideshowStates[containerId]].classList.add('active');
        }

        function currentSlide(element, index) {
            const container = element.closest('.slideshow-container');
            const containerId = container.id;
            
            if (!(containerId in slideshowStates)) {
                slideshowStates[containerId] = 0;
            }
            
            const slides = container.querySelectorAll('.slide');
            const dots = container.querySelectorAll('.dot');
            const currentIndex = slideshowStates[containerId];
            
            slides[currentIndex].classList.remove('active');
            dots[currentIndex].classList.remove('active');
            
            slideshowStates[containerId] = index;
            
            slides[index].classList.add('active');
            dots[index].classList.add('active');
        }
    </script>
</body>
</html>

